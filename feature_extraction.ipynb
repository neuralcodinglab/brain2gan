{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_aRYpLUgc5X"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://www.robots.ox.ac.uk/~vgg/software/vgg_face/src/vgg_face_torch.tar.gz\n",
        "! tar -xvf vgg_face_torch.tar.gz\n",
        "! pip install torchfile\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "xHoci5A0gvMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchfile\n",
        "from PIL import Image\n",
        "from scipy import signal"
      ],
      "metadata": {
        "id": "PMTzEYw3ghyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel(n):\n",
        "    x, y = np.meshgrid(np.linspace(-1, 1, n), np.linspace(-1, 1, n))\n",
        "    dst = np.sqrt(x*x + y*y)\n",
        "    sigma = 1\n",
        "    muu = 0.000\n",
        "    return np.exp(-((dst-muu) ** 2 / ( 2.0 * sigma**2)))\n",
        "\n",
        "\n",
        "def downsample(filters, layer):\n",
        "    ds = [56, 28]\n",
        "    result = np.zeros((filters.shape[1], ds[layer], ds[layer]))\n",
        "    for filter in range(filters.shape[1]):\n",
        "        result[filter] = signal.convolve(filters[0, filter], kernel(2), mode=\"same\")[::2, ::2]\n",
        "    return result[None]"
      ],
      "metadata": {
        "id": "35QLdY0MCaZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faces (StyleGAN3)"
      ],
      "metadata": {
        "id": "P0FDBqefgrsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG16 for face recognition"
      ],
      "metadata": {
        "id": "XbcmKtZ2me9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG_16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.block_size = [2, 2, 3, 3, 3]\n",
        "        self.conv_1_1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
        "        self.conv_1_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "        self.conv_2_1 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.conv_2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
        "        self.conv_3_1 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.conv_3_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.conv_3_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.conv_4_1 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "        self.conv_4_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_4_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_5_1 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.fc6 = nn.Linear(512 * 7 * 7, 4096)\n",
        "        self.fc7 = nn.Linear(4096, 4096)\n",
        "        self.fc8 = nn.Linear(4096, 2622)\n",
        "\n",
        "    def load_weights(self, path=\"/content/vgg_face_torch/VGG_FACE.t7\"):\n",
        "        model = torchfile.load(path)\n",
        "        counter = 1\n",
        "        block = 1\n",
        "        for i, layer in enumerate(model.modules):\n",
        "            if layer.weight is not None:\n",
        "                if block <= 5:\n",
        "                    self_layer = getattr(self, \"conv_%d_%d\" % (block, counter))\n",
        "                    counter += 1\n",
        "                    if counter > self.block_size[block - 1]:\n",
        "                        counter = 1\n",
        "                        block += 1\n",
        "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
        "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
        "                else:\n",
        "                    self_layer = getattr(self, \"fc%d\" % (block))\n",
        "                    block += 1\n",
        "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
        "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
        "\n",
        "    def forward(self, x, layer):\n",
        "        \"\"\" Pytorch forward\n",
        "        Args:\n",
        "            x: input image (224x224)\n",
        "        Returns: class logits\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv_1_1(x))\n",
        "        x = F.relu(self.conv_1_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        if layer == 1:\n",
        "            return x\n",
        "        x = F.relu(self.conv_2_1(x))\n",
        "        x = F.relu(self.conv_2_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        if layer == 2:\n",
        "            return x\n",
        "        x = F.relu(self.conv_3_1(x))\n",
        "        x = F.relu(self.conv_3_2(x))\n",
        "        x = F.relu(self.conv_3_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        if layer == 3:\n",
        "            return x\n",
        "        x = F.relu(self.conv_4_1(x))\n",
        "        x = F.relu(self.conv_4_2(x))\n",
        "        x = F.relu(self.conv_4_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        if layer == 4:\n",
        "            return x\n",
        "        x = F.relu(self.conv_5_1(x))\n",
        "        x = F.relu(self.conv_5_2(x))\n",
        "        x = F.relu(self.conv_5_3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        if layer == 5:\n",
        "            return x\n",
        "        # x = x.view(x.size(0), -1)\n",
        "        # x = F.relu(self.fc6(x))\n",
        "        # x = F.dropout(x, 0.5, self.training)\n",
        "        # if layer == 6:\n",
        "        #     return x\n",
        "        # x = F.relu(self.fc7(x))\n",
        "        # x = F.dropout(x, 0.5, self.training)\n",
        "        # if layer == 7:\n",
        "        #     return x\n",
        "        # return self.fc8(x)"
      ],
      "metadata": {
        "id": "zOY2xgyhgvUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelFace:\n",
        "    def __init__(self):\n",
        "        self.model = VGG_16().double()\n",
        "        self.model.load_weights()\n",
        "        self.model.eval().to('cuda')\n",
        "        self.ds = [56, 28]\n",
        "        \n",
        "    def preprocess(self, folder, split, n, start=0):\n",
        "        images = torch.zeros((split, 3, 224, 224))\n",
        "        for index, i in enumerate(range(split * n + start, split * (n + 1) + start)):\n",
        "            x = Image.open(\"/content/drive/My Drive/faces/%s/%s.png\" % (folder, str(i+1).zfill(4)))\n",
        "            x = np.asarray(x.resize((224, 224), resample=Image.LANCZOS)).astype(\"float32\")\n",
        "            x = torch.Tensor(x).permute(2, 0, 1).view(1, 3, 224, 224).double()\n",
        "            x -= torch.Tensor(np.array([93.5940, 104.7624, 129.1863])).double().view(1, 3, 1, 1)\n",
        "            images[index] = x\n",
        "        return images.to('cuda')\n",
        "\n",
        "    def get_features(self, layer, folder, split, n, start=0):\n",
        "        xs = self.preprocess(folder, split, n, start)\n",
        "        with torch.no_grad():\n",
        "            _out = self.model(xs.double(), layer).detach().cpu().numpy()\n",
        "            if index < 2:\n",
        "                ds = np.zeros((split, _out.shape[1], self.ds[layer-1], self.ds[layer-1]))\n",
        "                for i in range(_out.shape[0]):\n",
        "                    ds[i] = downsample(_out[i][None], layer-1).squeeze()\n",
        "                _out = ds\n",
        "            return _out"
      ],
      "metadata": {
        "id": "U7j4DQEDgvWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16 for face recognition\n",
        "vgg_face = ModelFace()\n",
        "layers = np.arange(1, 6)\n",
        "split = 50\n",
        "\n",
        "# test set features\n",
        "vgg_t1 = np.zeros((100, 64, 56, 56))\n",
        "vgg_t2 = np.zeros((100, 128, 56, 56))\n",
        "vgg_t3 = np.zeros((100, 256, 28, 28))\n",
        "vgg_t4 = np.zeros((100, 512, 14, 14))\n",
        "vgg_t5 = np.zeros((100, 512, 7, 7))\n",
        "vgg_faces = [vgg_t1, vgg_t2, vgg_t3, vgg_t4, vgg_t5]\n",
        "for i, layer in enumerate(layers):\n",
        "    for n in range(2):\n",
        "        vgg_faces[i][n*split:(n+1)*split] = vgg_face.get_features(layer, \"test\", split, n)\n",
        "    np.save(\"/content/drive/My Drive/faces/vggface_te_%i.npy\" % i, vgg_faces[i])\n",
        "\n",
        "# training set features\n",
        "vgg_t1 = np.zeros((4000, 64, 56, 56))  # ds: 112 -> 56\n",
        "vgg_t2 = np.zeros((4000, 128, 56, 56)) # ds: 56 -> 28\n",
        "vgg_t3 = np.zeros((4000, 256, 28, 28))\n",
        "vgg_t4 = np.zeros((4000, 512, 14, 14))\n",
        "vgg_t5 = np.zeros((4000, 512, 7, 7))\n",
        "vgg_faces = [vgg_t1, vgg_t2, vgg_t3, vgg_t4, vgg_t5]\n",
        "for i, layer in enumerate(layers):\n",
        "    for n in range(80):\n",
        "        vgg_faces[i][n*split:(n+1)*split] = vgg_face.get_features(layer, \"training\", split, n, 100)\n",
        "    np.save(\"/content/drive/My Drive/faces/vggface_tr_%i.npy\" % i, vgg_faces[i])"
      ],
      "metadata": {
        "id": "ox4vWC7jhCHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP (ViT-L/14@336px)"
      ],
      "metadata": {
        "id": "0vSL0T9amkk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-L/14@336px\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# test set\n",
        "images = []\n",
        "for i in range(100):\n",
        "    image = Image.open(\"/content/drive/My Drive/faces/test/%s.png\" % str(i+1).zfill(4))\n",
        "    images.append(preprocess(image))\n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "np.save(\"/content/drive/My Drive/faces/clip_te.npy\", image_features.cpu().detach().numpy())\n",
        "\n",
        "# training set\n",
        "images = []\n",
        "for i in range(4000): \n",
        "    image = Image.open(\"/content/drive/My Drive/faces/training/%s.png\" % str(i+101).zfill(4))\n",
        "    images.append(preprocess(image))\n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "np.save(\"/content/drive/My Drive/faces/clip_tr.npy\", image_features.cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "vNaBvgW8hCM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StyleGAN-XL (natural images)"
      ],
      "metadata": {
        "id": "svT-fEfFgmnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG16 for object recognition"
      ],
      "metadata": {
        "id": "_zEK4gsGnsix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, model, weights):\n",
        "        self.model = torch.hub.load('pytorch/vision:v0.10.0', model, weights=weights).eval().to('cuda')\n",
        "        self.preprocess = transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def get_features(self, index, layer, folder, split, n, start=0):\n",
        "        output = []\n",
        "        for i in range(start + split * n, start + split * (n + 1)):\n",
        "            input_image = Image.open(\"/content/drive/My Drive/images/%s/%s.png\" % (folder, str(i+1).zfill(4)))\n",
        "            input_tensor = self.preprocess(input_image).unsqueeze(0).to('cuda')\n",
        "            _out = self.model.features[:layer](input_tensor).detach().cpu().numpy()\n",
        "            if index < 2:\n",
        "                _out = downsample(_out, index)\n",
        "            output.append(_out)\n",
        "        return np.array(output).squeeze()"
      ],
      "metadata": {
        "id": "MU0Is3d8gh0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16 for object recognition\n",
        "vgg = Model(\"vgg16\", \"VGG16_Weights.DEFAULT\")\n",
        "layers = [5, 10, 17, 24, 31]\n",
        "split = 50\n",
        "\n",
        "# test set features\n",
        "vgg_t1 = np.zeros((200, 64, 56, 56))  # ds: 112 -> 56\n",
        "vgg_t2 = np.zeros((200, 128, 28, 28)) # ds: 56 -> 28\n",
        "vgg_t3 = np.zeros((200, 256, 28, 28))\n",
        "vgg_t4 = np.zeros((200, 512, 14, 14))\n",
        "vgg_t5 = np.zeros((200, 512, 7, 7))\n",
        "vgg = [vgg_t1, vgg_t2, vgg_t3, vgg_t4, vgg_t5]\n",
        "for i, layer in enumerate(layers):\n",
        "    for n in range(4):\n",
        "        vgg[i][n*split:(n+1)*split] = vgg.get_features(i, layer, \"test\", split=split, n=n)\n",
        "    np.save(\"/content/drive/My Drive/images/put/vgg_te_%i.npy\" % i, vgg[i])\n",
        "\n",
        "# training set features\n",
        "vgg_t1 = np.zeros((4000, 64, 56, 56))\n",
        "vgg_t2 = np.zeros((4000, 128, 28, 28))\n",
        "vgg_t3 = np.zeros((4000, 256, 28, 28))\n",
        "vgg_t4 = np.zeros((4000, 512, 14, 14))\n",
        "vgg_t5 = np.zeros((4000, 512, 7, 7))\n",
        "vgg = [vgg_t1, vgg_t2, vgg_t3, vgg_t4, vgg_t5]\n",
        "for i, layer in enumerate(layers):\n",
        "    for n in range(80):\n",
        "        vgg[i][n*split:(n+1)*split] = vgg.get_features(i, layer, \"training\", split=split, n=n, start=100)\n",
        "    np.save(\"/content/drive/My Drive/images/vgg_tr_%i.npy\" % i, vgg[i])"
      ],
      "metadata": {
        "id": "n3IkzRxlgh29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP (ViT-L/14@336px)"
      ],
      "metadata": {
        "id": "blACoM1qnpTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-L/14@336px\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# test set\n",
        "images = []\n",
        "for i in range(200):\n",
        "    image = Image.open(\"/content/drive/My Drive/images/test/%s.png\" % str(i+1).zfill(4))\n",
        "    images.append(preprocess(image))\n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "np.save(\"/content/drive/My Drive/images/clip_te.npy\", image_features.cpu().detach().numpy())\n",
        "\n",
        "# training set\n",
        "images = []\n",
        "for i in range(4000): \n",
        "    image = Image.open(\"/content/drive/My Drive/images/training/%s.png\" % str(i+101).zfill(4))\n",
        "    images.append(preprocess(image))\n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "np.save(\"/content/drive/My Drive/images/clip_tr.npy\", image_features.cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "nPxT44MMgh5P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}